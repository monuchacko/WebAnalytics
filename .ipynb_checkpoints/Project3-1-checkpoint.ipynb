{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3\n",
    "### Amanda Arce, Monu Chacko, Abdelmalek Hajjam, Nick Schettini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can.\n",
    "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev- test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import itertools\n",
    "from string import ascii_lowercase\n",
    "\n",
    "#nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "#shuffle the names\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let divide the data into test, dev and training datasets with 500, 500, x data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(names))\n",
    "#unpacking the names to 3 sets\n",
    "test, dev_test, training = names[:500], names[500:1000], names[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 1 extractor uses first letter, last letter and suffix as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features1(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data using Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 1 is: 0.788\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features1(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features1(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_1 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 1 is: \" + str(acc_dev_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 1 is: 0.778\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 1\n",
    "test_set = [(gender_features1(n), g) for (n,g) in test]\n",
    "test_set_1 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 1 is: \" + str(test_set_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 2 extractor uses first letter, last letter and two suffixes as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    features[\"suffix3\"] = name[-3:].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train feature 2 using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 2 is: 0.798\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features2(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features2(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_2 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 2 is: \" + str(acc_dev_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 2 is: 0.792\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 2\n",
    "test_set = [(gender_features2(n), g) for (n,g) in test]\n",
    "test_set_2 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 2 is: \" + str(test_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 3 extractor uses first letter, last letter and three suffixes as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features3(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    features[\"suffix3\"] = name[-3:].lower()\n",
    "    features[\"prefix3\"] = name[:3].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train feature 3 data using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 3 is: 0.824\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features3(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features3(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_3 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 3 is: \" + str(acc_dev_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 3 is: 0.806\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 3\n",
    "test_set = [(gender_features3(n), g) for (n,g) in test]\n",
    "test_set_3 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 3 is: \" + str(test_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features4(name):\n",
    "    \n",
    "    features = {}\n",
    "    keywords = [''.join(i) for i in itertools.product(ascii_lowercase, repeat = 2)]\n",
    "    \n",
    "    #look at first, first2, last, last2 letters of name\n",
    "    #apply .lower() method to convert all text to lowercase\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"first_2letter\"] = name[0:1].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    features[\"last_2letter\"] = name[-2:-1].lower()\n",
    "    \n",
    "    for letter in ascii_lowercase:\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "\n",
    "        for keyword in keywords:\n",
    "            features[\"combo2({})\".format(keyword)] = (keyword in name.lower())\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 3 is: 0.816\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features4(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features4(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_4 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 3 is: \" + str(acc_dev_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 4 is: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 4\n",
    "test_set = [(gender_features4(n), g) for (n,g) in test]\n",
    "test_set_4 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 4 is: \" + str(test_set_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(gender_features):\n",
    "    errors = []\n",
    "    for (name, tag) in dev_test:\n",
    "        guess = classifier.classify(gender_features(name))\n",
    "        if guess != tag:\n",
    "            errors.append((tag, guess, name))\n",
    "    print('no. of errors: ', len(errors))        \n",
    "        \n",
    "    #for (tag, guess, name) in sorted(errors): \n",
    "    #    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Bartolomeo'),\n",
       " ('male', 'female', 'Derk'),\n",
       " ('male', 'female', 'Archibald'),\n",
       " ('male', 'female', 'Benson'),\n",
       " ('male', 'female', 'Patin'),\n",
       " ('male', 'female', 'Wake'),\n",
       " ('male', 'female', 'Tristan'),\n",
       " ('male', 'female', 'Smith'),\n",
       " ('male', 'female', 'Tyson'),\n",
       " ('male', 'female', 'Ferinand')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst1 = error_analysis(gender_features1)\n",
    "lst1[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Bartolomeo'),\n",
       " ('male', 'female', 'Derk'),\n",
       " ('male', 'female', 'Archibald'),\n",
       " ('male', 'female', 'Benson'),\n",
       " ('male', 'female', 'Patin'),\n",
       " ('male', 'female', 'Wake'),\n",
       " ('male', 'female', 'Tristan'),\n",
       " ('male', 'female', 'Smith'),\n",
       " ('male', 'female', 'Tyson'),\n",
       " ('male', 'female', 'Ferinand')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst2 = error_analysis(gender_features2)\n",
    "lst2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Bartolomeo'),\n",
       " ('male', 'female', 'Derk'),\n",
       " ('male', 'female', 'Archibald'),\n",
       " ('male', 'female', 'Benson'),\n",
       " ('male', 'female', 'Patin'),\n",
       " ('male', 'female', 'Wake'),\n",
       " ('male', 'female', 'Tristan'),\n",
       " ('male', 'female', 'Smith'),\n",
       " ('male', 'female', 'Tyson'),\n",
       " ('male', 'female', 'Ferinand')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst3 = error_analysis(gender_features3)\n",
    "lst3[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('female', 'male', 'Trudy'),\n",
       " ('male', 'female', 'Patin'),\n",
       " ('male', 'female', 'Smith'),\n",
       " ('male', 'female', 'Ferinand'),\n",
       " ('female', 'male', 'Haley'),\n",
       " ('male', 'female', 'Grady'),\n",
       " ('female', 'male', 'Rosamond'),\n",
       " ('female', 'male', 'Goldie'),\n",
       " ('male', 'female', 'Dane'),\n",
       " ('female', 'male', 'Violet')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst4 = error_analysis(gender_features4)\n",
    "lst4[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 1: 0.788\n",
      "Accuracy Test Feature 1: 0.778\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 1: \" + str(acc_dev_test_1))\n",
    "print(\"Accuracy Test Feature 1: \" + str(test_set_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 2: 0.798\n",
      "Accuracy Test Feature 2: 0.792\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 2: \" + str(acc_dev_test_2))\n",
    "print(\"Accuracy Test Feature 2: \" + str(test_set_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 3: 0.824\n",
      "Accuracy Test Feature 3: 0.806\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 3: \" + str(acc_dev_test_3))\n",
    "print(\"Accuracy Test Feature 3: \" + str(test_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 4: 0.816\n",
      "Accuracy Test Feature 4: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 4: \" + str(acc_dev_test_4))\n",
    "print(\"Accuracy Test Feature 4: \" + str(test_set_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "#### - We found that feature 3 performed better than all the other features.\n",
    "#### - When comparing dev and test sets we found difference but were not significant. This was as expected.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
