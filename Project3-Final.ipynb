{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3\n",
    "### Amanda Arce, Monu Chacko, Abdelmalek Hajjam, Nick Schettini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can.\n",
    "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev- test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "#nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "#shuffle the names\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let divide the data into test, dev and training datasets with 500, 500, x data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(names))\n",
    "#unpacking the names to 3 sets\n",
    "test, dev_test, training = names[:500], names[500:1000], names[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 1 extractor uses first letter, last letter and suffix as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features1(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data using Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 1 is: 0.734\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features1(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features1(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_1 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 1 is: \" + str(acc_dev_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 1 is: 0.812\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 1\n",
    "test_set = [(gender_features1(n), g) for (n,g) in test]\n",
    "test_set_1 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 1 is: \" + str(test_set_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 2 extractor uses first letter, last letter and two suffixes as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    features[\"suffix3\"] = name[-3:].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train feature 2 using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 2 is: 0.744\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features2(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features2(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_2 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 2 is: \" + str(acc_dev_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 2 is: 0.824\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 2\n",
    "test_set = [(gender_features2(n), g) for (n,g) in test]\n",
    "test_set_2 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 2 is: \" + str(test_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gender feature 3 extractor uses first letter, last letter and three suffixes as its feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features3(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    features[\"suffix2\"] = name[-2:].lower()\n",
    "    features[\"suffix3\"] = name[-3:].lower()\n",
    "    features[\"prefix3\"] = name[:3].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train feature 3 data using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 3 is: 0.766\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features3(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features3(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_3 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 3 is: \" + str(acc_dev_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 3 is: 0.852\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 3\n",
    "test_set = [(gender_features3(n), g) for (n,g) in test]\n",
    "test_set_3 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 3 is: \" + str(test_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features4(name):\n",
    "    \n",
    "    features = {}\n",
    "    keywords = [''.join(i) for i in itertools.product(ascii_lowercase, repeat = 2)]\n",
    "    \n",
    "    #look at first, first2, last, last2 letters of name\n",
    "    #apply .lower() method to convert all text to lowercase\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"first_2letter\"] = name[0:1].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    features[\"last_2letter\"] = name[-2:-1].lower()\n",
    "    \n",
    "    for letter in ascii_lowercase:\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "\n",
    "        for keyword in keywords:\n",
    "            features[\"combo2({})\".format(keyword)] = (keyword in name.lower())\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dev using Feature 3 is: 0.772\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features4(n), g) for (n,g) in training]\n",
    "dev_test_set = [(gender_features4(n), g) for (n,g) in dev_test]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "acc_dev_test_4 = nltk.classify.accuracy(classifier, dev_test_set)\n",
    "print(\"The accuracy for the dev using Feature 3 is: \" + str(acc_dev_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test using Feature 4 is: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Performance test - Feature 4\n",
    "test_set = [(gender_features4(n), g) for (n,g) in test]\n",
    "test_set_4 = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"The accuracy for the test using Feature 4 is: \" + str(test_set_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(gender_features):\n",
    "    errors = []\n",
    "    for (name, tag) in dev_test:\n",
    "        guess = classifier.classify(gender_features(name))\n",
    "        if guess != tag:\n",
    "            errors.append((tag, guess, name))\n",
    "    print('no. of errors: ', len(errors))        \n",
    "        \n",
    "    #for (tag, guess, name) in sorted(errors): \n",
    "    #    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Kaspar'),\n",
       " ('male', 'female', 'Upton'),\n",
       " ('male', 'female', 'Wilber'),\n",
       " ('male', 'female', 'Erny'),\n",
       " ('male', 'female', 'Erl'),\n",
       " ('male', 'female', 'Say'),\n",
       " ('male', 'female', 'Serge'),\n",
       " ('male', 'female', 'Horace'),\n",
       " ('male', 'female', 'Efram'),\n",
       " ('male', 'female', 'Wolf')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst1 = error_analysis(gender_features1)\n",
    "lst1[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Kaspar'),\n",
       " ('male', 'female', 'Upton'),\n",
       " ('male', 'female', 'Wilber'),\n",
       " ('male', 'female', 'Erny'),\n",
       " ('male', 'female', 'Erl'),\n",
       " ('male', 'female', 'Say'),\n",
       " ('male', 'female', 'Serge'),\n",
       " ('male', 'female', 'Horace'),\n",
       " ('male', 'female', 'Efram'),\n",
       " ('male', 'female', 'Wolf')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst2 = error_analysis(gender_features2)\n",
    "lst2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('male', 'female', 'Kaspar'),\n",
       " ('male', 'female', 'Upton'),\n",
       " ('male', 'female', 'Wilber'),\n",
       " ('male', 'female', 'Erny'),\n",
       " ('male', 'female', 'Erl'),\n",
       " ('male', 'female', 'Say'),\n",
       " ('male', 'female', 'Serge'),\n",
       " ('male', 'female', 'Horace'),\n",
       " ('male', 'female', 'Efram'),\n",
       " ('male', 'female', 'Wolf')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst3 = error_analysis(gender_features3)\n",
    "lst3[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of errors:  114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('female', 'male', 'Row'),\n",
       " ('female', 'male', 'Patrice'),\n",
       " ('male', 'female', 'Erny'),\n",
       " ('female', 'male', 'Roz'),\n",
       " ('female', 'male', 'Sheree'),\n",
       " ('male', 'female', 'Kimball'),\n",
       " ('male', 'female', 'Cornellis'),\n",
       " ('male', 'female', 'Germaine'),\n",
       " ('female', 'male', 'Wynn'),\n",
       " ('male', 'female', 'Charlie')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst4 = error_analysis(gender_features4)\n",
    "lst4[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 1: 0.734\n",
      "Accuracy Test Feature 1: 0.812\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 1: \" + str(acc_dev_test_1))\n",
    "print(\"Accuracy Test Feature 1: \" + str(test_set_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 2: 0.744\n",
      "Accuracy Test Feature 2: 0.824\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 2: \" + str(acc_dev_test_2))\n",
    "print(\"Accuracy Test Feature 2: \" + str(test_set_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 3: 0.766\n",
      "Accuracy Test Feature 3: 0.852\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 3: \" + str(acc_dev_test_3))\n",
    "print(\"Accuracy Test Feature 3: \" + str(test_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dev Feature 4: 0.772\n",
      "Accuracy Test Feature 4: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Dev Feature 4: \" + str(acc_dev_test_4))\n",
    "print(\"Accuracy Test Feature 4: \" + str(test_set_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccuracySimulation(numIterations, callBackFunction):\n",
    "    acc_df = {\n",
    "        \"classifier\": [],\n",
    "        \"test_set_accuracy\": [],\n",
    "        \"dev_test_set_accuracy\": [],\n",
    "        \"train_set_accuracy\": [],\n",
    "        \"dev_test_errors\": []\n",
    "    }\n",
    "    for i in range(numIterations):\n",
    "        random.shuffle(names)\n",
    "        acc_train_names = names[1000:]\n",
    "        acc_dev_test_names = names[500:1000]\n",
    "        acc_test_names = names[:500]\n",
    "        acc_train_set = [(callBackFunction(n), g) for (n,g) in acc_train_names]\n",
    "        acc_dev_test_set = [(callBackFunction(n), g) for (n,g) in acc_dev_test_names]\n",
    "        acc_test_set = [(callBackFunction(n), g) for (n,g) in acc_test_names]\n",
    "        acc_classifier = nltk.NaiveBayesClassifier.train(acc_train_set)\n",
    "        acc_df[\"classifier\"].append(acc_classifier) \n",
    "        acc_df[\"test_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_test_set))\n",
    "        acc_df[\"dev_test_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_dev_test_set))\n",
    "        acc_df[\"train_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_train_set))\n",
    "       \n",
    "        acc_errors = []\n",
    "        for (name, tag) in acc_dev_test_names:\n",
    "            acc_guess = acc_classifier.classify(callBackFunction(name))\n",
    "            if acc_guess != tag:\n",
    "                acc_errors.append( (tag, acc_guess, name) )\n",
    "        acc_df[\"dev_test_errors\"].append(acc_errors)\n",
    "    acc_df = pd.DataFrame.from_dict(acc_df)\n",
    "    return(acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.791800</td>\n",
       "      <td>0.792600</td>\n",
       "      <td>0.800446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.023934</td>\n",
       "      <td>0.013794</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.797667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.799863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.800835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.801591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.801987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count          10.000000              10.000000           10.000000\n",
       "mean            0.791800               0.792600            0.800446\n",
       "std             0.023934               0.013794            0.001462\n",
       "min             0.762000               0.772000            0.797667\n",
       "25%             0.774000               0.780000            0.799863\n",
       "50%             0.789000               0.799000            0.800835\n",
       "75%             0.813500               0.803000            0.801591\n",
       "max             0.824000               0.808000            0.801987"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = AccuracySimulation(10, gender_features1)\n",
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.811800</td>\n",
       "      <td>0.800400</td>\n",
       "      <td>0.829349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.002241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.826757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.795500</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.827837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.828917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.830321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.833957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count          10.000000              10.000000           10.000000\n",
       "mean            0.811800               0.800400            0.829349\n",
       "std             0.023598               0.015771            0.002241\n",
       "min             0.776000               0.768000            0.826757\n",
       "25%             0.795500               0.793000            0.827837\n",
       "50%             0.816000               0.804000            0.828917\n",
       "75%             0.826000               0.812500            0.830321\n",
       "max             0.844000               0.818000            0.833957"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = AccuracySimulation(10, gender_features2)\n",
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.863292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.013533</td>\n",
       "      <td>0.017338</td>\n",
       "      <td>0.001368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.861751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.818500</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.862111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.863191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.863947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.866071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count          10.000000              10.000000           10.000000\n",
       "mean            0.829400               0.823200            0.863292\n",
       "std             0.013533               0.017338            0.001368\n",
       "min             0.812000               0.800000            0.861751\n",
       "25%             0.818500               0.810500            0.862111\n",
       "50%             0.828000               0.821000            0.863191\n",
       "75%             0.841500               0.829000            0.863947\n",
       "max             0.850000               0.852000            0.866071"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3 = AccuracySimulation(10, gender_features3)\n",
    "df_3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>dev_test_set_accuracy</th>\n",
       "      <th>train_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.804200</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.815294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.018961</td>\n",
       "      <td>0.021066</td>\n",
       "      <td>0.003028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.811492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.812680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.815668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.817216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.820853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_set_accuracy  dev_test_set_accuracy  train_set_accuracy\n",
       "count          10.000000              10.000000           10.000000\n",
       "mean            0.804200               0.803000            0.815294\n",
       "std             0.018961               0.021066            0.003028\n",
       "min             0.768000               0.754000            0.811492\n",
       "25%             0.796500               0.797000            0.812680\n",
       "50%             0.802000               0.807000            0.815668\n",
       "75%             0.816000               0.817500            0.817216\n",
       "max             0.836000               0.824000            0.820853"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4 = AccuracySimulation(10, gender_features4)\n",
    "df_4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "#### - We found that feature 3 performed better than all the other features.\n",
    "#### - When comparing dev and test sets we found difference but were not significant. This was as expected.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
